# Proyecto Final - Soy Henry - Data Analytics

Este proyecto corresponde al trabajo final del bootcamp de Data Analytics de Soy Henry. El objetivo es realizar un anÃ¡lisis de datos de compras y ventas del aÃ±o 2016, aplicando tÃ©cnicas de exploraciÃ³n, limpieza y modelado con Python.

## ğŸ“ Estructura del proyecto

- `proyecto_final.ipynb`: Notebook principal con el anÃ¡lisis completo.
- `*.csv`: Archivos con los datos utilizados para el anÃ¡lisis.
- `.gitignore`: Configurado para excluir archivos innecesarios del repositorio.
- `requirements.txt`: Lista de bibliotecas necesarias para ejecutar el proyecto.
- `nanclajecloud` Archivo relacionado con la configuraciÃ³n o integraciÃ³n en entorno cloud



> âš ï¸ **RecomendaciÃ³n:** Mover los archivos `.csv` a una carpeta `data/` y excluir esa carpeta del repositorio para mantenerlo liviano.

## ğŸ“Š Datos utilizados

- `2017PurchasePricesDec.csv`
- `BegInvFINAL12312016.csv`
- `EndInvFINAL12312016.csv`
- `InvoicePurchases12312016.csv`
- `PurchasesFINAL12312016.csv`
- `SalesFINAL12312016.csv`
- `data_final2.xlsx`

## ğŸ§ª TecnologÃ­as y librerÃ­as utilizadas

- Python 3.13 (o compatible)
- Jupyter Notebook
- Pandas
- NumPy
- Matplotlib
- Scikit-learn
- Category Encoders

## â–¶ï¸ CÃ³mo ejecutar el proyecto

1. Clonar este repositorio:
   ```bash
   git clone https://github.com/NoeliaGCalligaro/proyectofinal.git
   cd proyecto final 

2. Crear y activar un entorno virtual:
   python -m venv venv
   # Activar en Windows
   .\venv\Scripts\activate
   # Activar en Mac/Linux
   source venv/bin/activate



3. Instalar las dependencias:

    pip install -r requirements.txt



ğŸ“ˆ Contenido de Anclaje_cloud.ipynb
â˜ï¸ ANCLAJE CLOUD - CARGA Y CONFIRMACIÃ“N DE DATOS EN BIGQUERY

ğŸ‘‰ 1. IMPORTACIÃ“N DE LIBRERÃAS
ğŸ“Œ Se importa pandas como librerÃ­a base.

ğŸ‘‰ 2. DEFINICIÃ“N DE NUEVOS DATOS A CARGAR
ğŸ“Œ Se generan listas con datos simulados o nuevos para cargar en la base de datos.
â€ƒâ€ƒEjemplos:
â€ƒâ€ƒ- datos_new_data1 â†’ Producto
â€ƒâ€ƒ- datos_new_data2 â†’ Inventario inicial
â€ƒâ€ƒ- datos_new_data3 â†’ Inventario final
â€ƒâ€ƒ- datos_new_data4 â†’ Compras
â€ƒâ€ƒ- datos_new_data5 â†’ Detalle de compras
â€ƒâ€ƒ- datos_new_data6 â†’ Ventas

ğŸ‘‰ 3. CARGA DE DATOS A BIGQUERY
ğŸ“Œ Se utiliza la funciÃ³n cargar_datos_una_tabla para insertar datos en tablas especÃ­ficas dentro del proyecto y dataset:
â€ƒâ€ƒ- Proyecto: soy-henry-459003
â€ƒâ€ƒ- Dataset: andes_insight
â€ƒâ€ƒTablas afectadas:
â€ƒâ€ƒ- Productos
â€ƒâ€ƒ- Inventario_inicial
â€ƒâ€ƒ- Inventario_final
â€ƒâ€ƒ- Compras
â€ƒâ€ƒ- Detalle_compras
â€ƒâ€ƒ- Ventas

ğŸ‘‰ 4. CONFIRMACIÃ“N DE DATOS CARGADOS
ğŸ“Œ Se usa la funciÃ³n confirmar_nuevos_datos_cargados para verificar que los datos se hayan insertado correctamente en cada tabla

ğŸ‘‰ 5. ELIMINACIÃ“N DE DATOS CARGADOS
ğŸ“Œ Se eliminan los datos previamente insertados en cada tabla utilizando la funciÃ³n eliminar_datos_cargados_por_columna.
â€ƒâ€ƒTablas afectadas:
â€ƒâ€ƒ- Productos
â€ƒâ€ƒ- Inventario_inicial
â€ƒâ€ƒ- Inventario_final
â€ƒâ€ƒ- Compras
â€ƒâ€ƒ- Detalle_compras
â€ƒâ€ƒ- Ventas

ğŸ‘‰ 6. CONFIRMACIÃ“N DE ELIMINACIÃ“N DE DATOS
ğŸ“Œ ValidaciÃ³n con confirmar_nuevos_datos_cargados para asegurar que los registros eliminados ya no se encuentren en las tablas.

ğŸ“ˆ Contenido de proyecto_final.ipynb
ğŸ“¥ FUENTE DE DATOS
Antes del proceso de ETL, se toman los datos directamente desde la base de datos andes_insight, donde se encuentra el dataset previamente cargado y listo para trabajar.

ğŸ“ˆ ANALISIS GENERAL DEL DATASET
      ğŸ‘‰ 1. IMPORTAMOS LIBRERIAS
      ğŸ‘‰ 2. ABRIMOS DATASET
      ğŸ‘‰ 3. EDA - ANALISIS EXPLORATORIO DE DATOS EDA - INCIAL
           ğŸ“Œ A. AnÃ¡lisis Integral EDA  (dimension, columnas, tipo de columnas, nulos, estadÃ­sticas variables numericas, detalle de categÃ³ricas) 
           ğŸ“Œ B. AnÃ¡lisis de Columnas con alta variabilidad
           ğŸ“Œ C. AnÃ¡lisis de Columnas con valores nÃºmericos ceros
      ğŸ‘‰ 4. ETL - PROCESO DE EXTRACCION, TRANSFORMACION Y LIMPIEZA DE DATOS
           ğŸ“Œ A. Remplazamos valores nulos
           ğŸ“Œ B. Remplazamos valores equivalente a cero en columnas nÃºmericas, cuando es necesario
           ğŸ“Œ C. Cambiamos tipo de datos en variables catÃ©goricas que se encuentran como nÃºmericas (Brand, Store,VendorNumber)
      ğŸ‘‰ 5. DETERMINAR STOCK OPTIMO
           ğŸ“Œ A. Definimos funcion de stock Ã³ptimo (Stock Ã³ptimo = Demanda diaria promedio Ã— Tiempo de reposiciÃ³n promedio + Stock de seguridad) 
           ğŸ“Œ B. Determinamos tiempo de reposiÃ³n, y determinar media y desviacion std del tiempo de reposiciÃ³n medio para cada IventoryID 
           ğŸ“Œ C. Determinamos demanda diaria, y determinar media y desviacion std de la demanda diaria para cada IventoryID
           ğŸ“Œ D. Unimos en una tabla Inventario Incial, Inventario Final, Tiempo de ReposiciÃ³n y Demanda Diaria
           ğŸ“Œ E. Estimamos tiempo de reposiciÃ³n para productos que no han sido comprados, y Estimar demanda diaria para productos que no han sido                         vendidos. La estimaciÃ³n la haremos con tÃ©cnicas de Maching Learning (Random Forest).
           ğŸ“Œ F. Determinamos Demanda diaria media. Analizar si se utiliza la demanda diaria media obtenida con Maching Learning, o la funciÃ³n de                        demanda diaria media obtenida con la funcion: Demada Estimada =Inventario Inicial + Compras - Inventario Final
           ğŸ“Œ G  Determimos Stock Optimo y Demanda Estimada
           ğŸ“Œ H. Exportamos nueva tabla Stock Ã“ptimo
      ğŸ‘‰ 6. EDA - ANALISIS EXPLORATORIO DE DATOS EDA - FINAL


## Autores
- **Amalia Granata** 
- **Jenny Ortiz** 
- **Luis Ladino** 
- **Noelia Calligaro** 
